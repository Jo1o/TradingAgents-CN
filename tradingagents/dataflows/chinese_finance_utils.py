#!/usr/bin/env python3
"""
ä¸­å›½è´¢ç»æ•°æ®èšåˆå·¥å…·
ç”±äºå¾®åšAPIç”³è¯·å›°éš¾ä¸”åŠŸèƒ½å—é™ï¼Œé‡‡ç”¨å¤šæºæ•°æ®èšåˆçš„æ–¹å¼
"""

import requests
import json
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import re
from bs4 import BeautifulSoup
import pandas as pd


class ChineseFinanceDataAggregator:
    """ä¸­å›½è´¢ç»æ•°æ®èšåˆå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def get_stock_sentiment_summary(self, ticker: str, days: int = 7) -> Dict:
        """
        è·å–è‚¡ç¥¨æƒ…ç»ªåˆ†ææ±‡æ€»
        æ•´åˆå¤šä¸ªå¯è·å–çš„ä¸­å›½è´¢ç»æ•°æ®æº
        """
        try:
            # 1. è·å–è´¢ç»æ–°é—»æƒ…ç»ª
            news_sentiment = self._get_finance_news_sentiment(ticker, days)
            
            # 2. è·å–è‚¡å§è®¨è®ºçƒ­åº¦ (å¦‚æœå¯ä»¥è·å–)
            forum_sentiment = self._get_stock_forum_sentiment(ticker, days)
            
            # 3. è·å–è´¢ç»åª’ä½“æŠ¥é“
            media_sentiment = self._get_media_coverage_sentiment(ticker, days)
            
            # 4. ç»¼åˆåˆ†æ
            overall_sentiment = self._calculate_overall_sentiment(
                news_sentiment, forum_sentiment, media_sentiment
            )
            
            return {
                'ticker': ticker,
                'analysis_period': f'{days} days',
                'overall_sentiment': overall_sentiment,
                'news_sentiment': news_sentiment,
                'forum_sentiment': forum_sentiment,
                'media_sentiment': media_sentiment,
                'summary': self._generate_sentiment_summary(overall_sentiment),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                'ticker': ticker,
                'error': f'æ•°æ®è·å–å¤±è´¥: {str(e)}',
                'fallback_message': 'ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“APIé™åˆ¶ï¼Œå»ºè®®ä½¿ç”¨è´¢ç»æ–°é—»å’ŒåŸºæœ¬é¢åˆ†æä½œä¸ºä¸»è¦å‚è€ƒ',
                'timestamp': datetime.now().isoformat()
            }
    
    def _get_finance_news_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–è´¢ç»æ–°é—»æƒ…ç»ªåˆ†æ"""
        try:
            # æœç´¢ç›¸å…³æ–°é—»æ ‡é¢˜å’Œå†…å®¹
            company_name = self._get_company_chinese_name(ticker)
            search_terms = [ticker, company_name] if company_name else [ticker]
            
            news_items = []
            for term in search_terms:
                # è¿™é‡Œå¯ä»¥é›†æˆå¤šä¸ªæ–°é—»æº
                items = self._search_finance_news(term, days)
                news_items.extend(items)
            
            # ç®€å•çš„æƒ…ç»ªåˆ†æ
            positive_count = 0
            negative_count = 0
            neutral_count = 0
            
            for item in news_items:
                sentiment = self._analyze_text_sentiment(item.get('title', '') + ' ' + item.get('content', ''))
                if sentiment > 0.1:
                    positive_count += 1
                elif sentiment < -0.1:
                    negative_count += 1
                else:
                    neutral_count += 1
            
            total = len(news_items)
            if total == 0:
                return {'sentiment_score': 0, 'confidence': 0, 'news_count': 0}
            
            sentiment_score = (positive_count - negative_count) / total
            
            return {
                'sentiment_score': sentiment_score,
                'positive_ratio': positive_count / total,
                'negative_ratio': negative_count / total,
                'neutral_ratio': neutral_count / total,
                'news_count': total,
                'confidence': min(total / 10, 1.0)  # æ–°é—»æ•°é‡è¶Šå¤šï¼Œç½®ä¿¡åº¦è¶Šé«˜
            }
            
        except Exception as e:
            return {'error': str(e), 'sentiment_score': 0, 'confidence': 0}
    
    def _get_stock_forum_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–è‚¡ç¥¨è®ºå›è®¨è®ºæƒ…ç»ª (æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…éœ€è¦çˆ¬è™«)"""
        # ç”±äºä¸œæ–¹è´¢å¯Œè‚¡å§ç­‰å¹³å°çš„åçˆ¬è™«æœºåˆ¶ï¼Œè¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        # å®é™…å®ç°éœ€è¦æ›´å¤æ‚çš„çˆ¬è™«æŠ€æœ¯
        
        return {
            'sentiment_score': 0,
            'discussion_count': 0,
            'hot_topics': [],
            'note': 'è‚¡ç¥¨è®ºå›æ•°æ®è·å–å—é™ï¼Œå»ºè®®å…³æ³¨å®˜æ–¹è´¢ç»æ–°é—»',
            'confidence': 0
        }
    
    def _get_media_coverage_sentiment(self, ticker: str, days: int) -> Dict:
        """è·å–åª’ä½“æŠ¥é“æƒ…ç»ª"""
        try:
            # å¯ä»¥é›†æˆRSSæºæˆ–å…¬å¼€çš„è´¢ç»API
            coverage_items = self._get_media_coverage(ticker, days)
            
            if not coverage_items:
                return {'sentiment_score': 0, 'coverage_count': 0, 'confidence': 0}
            
            # åˆ†æåª’ä½“æŠ¥é“çš„æƒ…ç»ªå€¾å‘
            sentiment_scores = []
            for item in coverage_items:
                score = self._analyze_text_sentiment(item.get('title', '') + ' ' + item.get('summary', ''))
                sentiment_scores.append(score)
            
            avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
            
            return {
                'sentiment_score': avg_sentiment,
                'coverage_count': len(coverage_items),
                'confidence': min(len(coverage_items) / 5, 1.0)
            }
            
        except Exception as e:
            return {'error': str(e), 'sentiment_score': 0, 'confidence': 0}
    
    def _search_finance_news(self, search_term: str, days: int) -> List[Dict]:
        """æœç´¢è´¢ç»æ–°é—» - é›†æˆæ–°é—»åˆ†æå¸ˆæ•°æ®"""
        try:
            # ä½¿ç”¨æ–°é—»åˆ†æå¸ˆçš„ç»Ÿä¸€æ–°é—»å·¥å…·è·å–çœŸå®æ–°é—»æ•°æ®
            from tradingagents.agents.utils.agent_utils import Toolkit
            from datetime import datetime, timedelta
            
            # è®¡ç®—æŸ¥è¯¢æ—¥æœŸ
            end_date = datetime.now()
            curr_date_str = end_date.strftime('%Y-%m-%d')
            
            # åˆ›å»ºå·¥å…·åŒ…å®ä¾‹
            toolkit = Toolkit()
            
            # å°è¯•è·å–Googleæ–°é—»ï¼ˆé€‚ç”¨äºä¸­æ–‡æœç´¢ï¼‰
            try:
                news_data = toolkit.get_google_news(search_term, curr_date_str)
                # è§£ææ–°é—»æ•°æ®å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                return self._parse_news_data(news_data, search_term)
            except Exception as e:
                print(f"Googleæ–°é—»è·å–å¤±è´¥: {e}")
                # å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®
                return [{
                    'title': f'{search_term}ç›¸å…³è´¢ç»æ–°é—»',
                    'content': 'æš‚æ— æ–°é—»å†…å®¹ï¼Œæ•°æ®æºè¿æ¥å¤±è´¥',
                    'source': 'æ•°æ®æºä¸å¯ç”¨',
                    'publish_time': datetime.now().isoformat(),
                    'url': '#'
                }]
                
        except Exception as e:
            print(f"æ–°é—»è·å–å¼‚å¸¸: {e}")
            return []
    
    def _get_media_coverage(self, ticker: str, days: int) -> List[Dict]:
        """è·å–åª’ä½“æŠ¥é“ - é›†æˆç»Ÿä¸€æ–°é—»å·¥å…·"""
        try:
            # ä½¿ç”¨ç»Ÿä¸€æ–°é—»å·¥å…·è·å–è‚¡ç¥¨ç›¸å…³åª’ä½“æŠ¥é“
            from tradingagents.agents.utils.agent_utils import Toolkit
            from datetime import datetime
            
            curr_date_str = datetime.now().strftime('%Y-%m-%d')
            toolkit = Toolkit()
            
            # ä½¿ç”¨ç»Ÿä¸€æ–°é—»å·¥å…·è·å–è‚¡ç¥¨æ–°é—»
            try:
                news_data = toolkit.get_stock_news_unified(ticker, curr_date_str)
                return self._parse_news_data(news_data, ticker)
            except Exception as e:
                print(f"ç»Ÿä¸€æ–°é—»å·¥å…·è·å–å¤±è´¥: {e}")
                return []
                
        except Exception as e:
            print(f"åª’ä½“æŠ¥é“è·å–å¼‚å¸¸: {e}")
            return []
    
    def _parse_news_data(self, news_data: str, search_term: str) -> List[Dict]:
        """è§£ææ–°é—»æ•°æ®å¹¶è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼"""
        try:
            # ç®€å•è§£ææ–°é—»æ–‡æœ¬æ•°æ®
            # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…è¿”å›çš„æ•°æ®æ ¼å¼è¿›è¡Œè°ƒæ•´
            if not news_data or news_data.strip() == "":
                return []
            
            # å°†æ–°é—»æ•°æ®åˆ†å‰²æˆæ®µè½ï¼Œæ¯ä¸ªæ®µè½ä½œä¸ºä¸€æ¡æ–°é—»
            news_lines = [line.strip() for line in news_data.split('\n') if line.strip()]
            parsed_news = []
            
            for i, line in enumerate(news_lines[:5]):  # é™åˆ¶æœ€å¤š5æ¡æ–°é—»
                if len(line) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„è¡Œ
                    parsed_news.append({
                        'title': line[:100] + '...' if len(line) > 100 else line,
                        'content': line,
                        'source': 'æ–°é—»èšåˆ',
                        'publish_time': datetime.now().isoformat(),
                        'url': f'#news_{i}'
                    })
            
            return parsed_news if parsed_news else [{
                'title': f'{search_term}ç›¸å…³æ–°é—»',
                'content': news_data[:200] + '...' if len(news_data) > 200 else news_data,
                'source': 'æ–°é—»èšåˆ',
                'publish_time': datetime.now().isoformat(),
                'url': '#'
            }]
            
        except Exception as e:
            print(f"æ–°é—»æ•°æ®è§£æå¤±è´¥: {e}")
            return []
    
    def _analyze_text_sentiment(self, text: str) -> float:
        """ç®€å•çš„ä¸­æ–‡æ–‡æœ¬æƒ…ç»ªåˆ†æ"""
        if not text:
            return 0
        
        # ç®€å•çš„å…³é”®è¯æƒ…ç»ªåˆ†æ
        positive_words = ['ä¸Šæ¶¨', 'å¢é•¿', 'åˆ©å¥½', 'çœ‹å¥½', 'ä¹°å…¥', 'æ¨è', 'å¼ºåŠ¿', 'çªç ´', 'åˆ›æ–°é«˜']
        negative_words = ['ä¸‹è·Œ', 'ä¸‹é™', 'åˆ©ç©º', 'çœ‹ç©º', 'å–å‡º', 'é£é™©', 'è·Œç ´', 'åˆ›æ–°ä½', 'äºæŸ']
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count + negative_count == 0:
            return 0
        
        return (positive_count - negative_count) / (positive_count + negative_count)
    
    def _get_company_chinese_name(self, ticker: str) -> Optional[str]:
        """è·å–å…¬å¸ä¸­æ–‡åç§°"""
        # ç®€å•çš„æ˜ å°„è¡¨ï¼Œå®é™…å¯ä»¥ä»æ•°æ®åº“æˆ–APIè·å–
        name_mapping = {
            'AAPL': 'è‹¹æœ',
            'TSLA': 'ç‰¹æ–¯æ‹‰',
            'NVDA': 'è‹±ä¼Ÿè¾¾',
            'MSFT': 'å¾®è½¯',
            'GOOGL': 'è°·æ­Œ',
            'AMZN': 'äºšé©¬é€Š'
        }
        return name_mapping.get(ticker.upper())
    
    def _calculate_overall_sentiment(self, news_sentiment: Dict, forum_sentiment: Dict, media_sentiment: Dict) -> Dict:
        """è®¡ç®—ç»¼åˆæƒ…ç»ªåˆ†æ"""
        # æ ¹æ®å„æ•°æ®æºçš„ç½®ä¿¡åº¦åŠ æƒè®¡ç®—
        news_weight = news_sentiment.get('confidence', 0)
        forum_weight = forum_sentiment.get('confidence', 0)
        media_weight = media_sentiment.get('confidence', 0)
        
        total_weight = news_weight + forum_weight + media_weight
        
        if total_weight == 0:
            return {'sentiment_score': 0, 'confidence': 0, 'level': 'neutral'}
        
        weighted_sentiment = (
            news_sentiment.get('sentiment_score', 0) * news_weight +
            forum_sentiment.get('sentiment_score', 0) * forum_weight +
            media_sentiment.get('sentiment_score', 0) * media_weight
        ) / total_weight
        
        # ç¡®å®šæƒ…ç»ªç­‰çº§
        if weighted_sentiment > 0.3:
            level = 'very_positive'
        elif weighted_sentiment > 0.1:
            level = 'positive'
        elif weighted_sentiment > -0.1:
            level = 'neutral'
        elif weighted_sentiment > -0.3:
            level = 'negative'
        else:
            level = 'very_negative'
        
        return {
            'sentiment_score': weighted_sentiment,
            'confidence': total_weight / 3,  # å¹³å‡ç½®ä¿¡åº¦
            'level': level
        }
    
    def _generate_sentiment_summary(self, overall_sentiment: Dict) -> str:
        """ç”Ÿæˆæƒ…ç»ªåˆ†ææ‘˜è¦"""
        level = overall_sentiment.get('level', 'neutral')
        score = overall_sentiment.get('sentiment_score', 0)
        confidence = overall_sentiment.get('confidence', 0)
        
        level_descriptions = {
            'very_positive': 'éå¸¸ç§¯æ',
            'positive': 'ç§¯æ',
            'neutral': 'ä¸­æ€§',
            'negative': 'æ¶ˆæ',
            'very_negative': 'éå¸¸æ¶ˆæ'
        }
        
        description = level_descriptions.get(level, 'ä¸­æ€§')
        confidence_level = 'é«˜' if confidence > 0.7 else 'ä¸­' if confidence > 0.3 else 'ä½'
        
        return f"å¸‚åœºæƒ…ç»ª: {description} (è¯„åˆ†: {score:.2f}, ç½®ä¿¡åº¦: {confidence_level})"


def get_chinese_social_sentiment(ticker: str, curr_date: str) -> str:
    """
    è·å–ä¸­å›½ç¤¾äº¤åª’ä½“æƒ…ç»ªåˆ†æçš„ä¸»è¦æ¥å£å‡½æ•°
    """
    aggregator = ChineseFinanceDataAggregator()
    
    try:
        # è·å–æƒ…ç»ªåˆ†ææ•°æ®
        sentiment_data = aggregator.get_stock_sentiment_summary(ticker, days=7)
        
        # æ ¼å¼åŒ–è¾“å‡º
        if 'error' in sentiment_data:
            return f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âš ï¸ æ•°æ®è·å–é™åˆ¶è¯´æ˜:
{sentiment_data.get('fallback_message', 'æ•°æ®è·å–é‡åˆ°æŠ€æœ¯é™åˆ¶')}

å»ºè®®:
1. é‡ç‚¹å…³æ³¨è´¢ç»æ–°é—»å’ŒåŸºæœ¬é¢åˆ†æ
2. å‚è€ƒå®˜æ–¹è´¢æŠ¥å’Œä¸šç»©æŒ‡å¯¼
3. å…³æ³¨è¡Œä¸šæ”¿ç­–å’Œç›‘ç®¡åŠ¨æ€
4. è€ƒè™‘å›½é™…å¸‚åœºæƒ…ç»ªå¯¹ä¸­æ¦‚è‚¡çš„å½±å“

æ³¨: ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“å¹³å°APIé™åˆ¶ï¼Œå½“å‰ä¸»è¦ä¾èµ–å…¬å¼€è´¢ç»æ•°æ®æºè¿›è¡Œåˆ†æã€‚
"""
        
        overall = sentiment_data.get('overall_sentiment', {})
        news = sentiment_data.get('news_sentiment', {})
        
        return f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}
åˆ†æå‘¨æœŸ: {sentiment_data.get('analysis_period', '7å¤©')}

ğŸ“Š ç»¼åˆæƒ…ç»ªè¯„ä¼°:
{sentiment_data.get('summary', 'æ•°æ®ä¸è¶³')}

ğŸ“° è´¢ç»æ–°é—»æƒ…ç»ª:
- æƒ…ç»ªè¯„åˆ†: {news.get('sentiment_score', 0):.2f}
- æ­£é¢æ–°é—»æ¯”ä¾‹: {news.get('positive_ratio', 0):.1%}
- è´Ÿé¢æ–°é—»æ¯”ä¾‹: {news.get('negative_ratio', 0):.1%}
- æ–°é—»æ•°é‡: {news.get('news_count', 0)}æ¡

ğŸ’¡ æŠ•èµ„å»ºè®®:
åŸºäºå½“å‰å¯è·å–çš„ä¸­å›½å¸‚åœºæ•°æ®ï¼Œå»ºè®®æŠ•èµ„è€…:
1. å¯†åˆ‡å…³æ³¨å®˜æ–¹è´¢ç»åª’ä½“æŠ¥é“
2. é‡è§†åŸºæœ¬é¢åˆ†æå’Œè´¢åŠ¡æ•°æ®
3. è€ƒè™‘æ”¿ç­–ç¯å¢ƒå¯¹è‚¡ä»·çš„å½±å“
4. å…³æ³¨å›½é™…å¸‚åœºåŠ¨æ€

âš ï¸ æ•°æ®è¯´æ˜:
ç”±äºä¸­å›½ç¤¾äº¤åª’ä½“å¹³å°APIè·å–é™åˆ¶ï¼Œæœ¬åˆ†æä¸»è¦åŸºäºå…¬å¼€è´¢ç»æ–°é—»æ•°æ®ã€‚
å»ºè®®ç»“åˆå…¶ä»–åˆ†æç»´åº¦è¿›è¡Œç»¼åˆåˆ¤æ–­ã€‚

ç”Ÿæˆæ—¶é—´: {sentiment_data.get('timestamp', datetime.now().isoformat())}
"""
        
    except Exception as e:
        return f"""
ä¸­å›½å¸‚åœºæƒ…ç»ªåˆ†æ - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âŒ åˆ†æå¤±è´¥: {str(e)}

ğŸ’¡ æ›¿ä»£å»ºè®®:
1. æŸ¥çœ‹è´¢ç»æ–°é—»ç½‘ç«™çš„ç›¸å…³æŠ¥é“
2. å…³æ³¨é›ªçƒã€ä¸œæ–¹è´¢å¯Œç­‰æŠ•èµ„ç¤¾åŒºè®¨è®º
3. å‚è€ƒä¸“ä¸šæœºæ„çš„ç ”ç©¶æŠ¥å‘Š
4. é‡ç‚¹åˆ†æåŸºæœ¬é¢å’ŒæŠ€æœ¯é¢æ•°æ®

æ³¨: ä¸­å›½ç¤¾äº¤åª’ä½“æ•°æ®è·å–å­˜åœ¨æŠ€æœ¯é™åˆ¶ï¼Œå»ºè®®ä»¥åŸºæœ¬é¢åˆ†æä¸ºä¸»ã€‚
"""
